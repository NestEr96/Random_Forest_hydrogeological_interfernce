"""
CODE 1
-------------------------------------------------------------------------------
Random Forest vs DHI indices on TAV_Machine dataset.
Includes bootstrap comparison and ROC analysis.
"""

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import joblib

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef,
    roc_curve,
    roc_auc_score
)
from sklearn.inspection import permutation_importance

# -------------------------------
# Settings
# -------------------------------
TEST_SIZE = 0.30
RANDOM_STATES = range(1, 101)
N_BOOTSTRAP = 10_000
ALPHA = 0.05
mean_fpr = np.linspace(0, 1, 100)

# -------------------------------
# Data loading
# -------------------------------
df = pd.read_csv("TAV_Machine.csv", sep=";")

target = "Impatto"
y_true = df[target].values

X = df.drop(columns=[
    target, "Numero", "DHI1", "DHI2",
    "Spring group", "A/P", "Qm (L/s)", "Tipo", 
    "Fault distance (m)", "Fault distance 2 (m)"
])

# -------------------------------
# Containers
# -------------------------------
y_pred_rf_list = []
test_indices_list = []

tpr_list = []
auc_list = []

acc_list = []
prec_list = []
rec_list = []
f1_list = []
mcc_list = []

pfi_list = []              
shap_values_list = []    
models_list = []  

rf_prob_list = []
rf_models = []           

feature_names = X.columns
# -------------------------------
# Random Forest training loop
# -------------------------------
for rs in RANDOM_STATES:

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_true,
        test_size=TEST_SIZE,
        stratify=y_true,
        random_state=rs
    )

    model = RandomForestClassifier(
        random_state=rs,
        max_depth=8,
        n_estimators=490,
        max_features="sqrt",
        min_samples_leaf=1,
        min_samples_split=2
    )

    model.fit(X_train, y_train)
    rf_models.append(model)

    # Predictions
    y_pred = model.predict(X_test)
    y_pred_rf_list.append(y_pred)
    test_indices_list.append(X_test.index)
    
    # --- Classification metrics ---  
    acc_list.append(accuracy_score(y_test, y_pred))
    prec_list.append(precision_score(y_test, y_pred))
    rec_list.append(recall_score(y_test, y_pred))
    f1_list.append(f1_score(y_test, y_pred))
    mcc_list.append(matthews_corrcoef(y_test, y_pred))

    # --- ROC computation for current split ---  
    y_scores = model.predict_proba(X_test)[:, 1]

    fpr, tpr, _ = roc_curve(y_test, y_scores)
    roc_auc = roc_auc_score(y_test, y_scores)

    # Interpolate TPR over a common FPR grid
    tpr_interp = np.interp(mean_fpr, fpr, tpr)
    tpr_interp[0] = 0.0

    tpr_list.append(tpr_interp)
    auc_list.append(roc_auc)

    # ---------------------------------
    # Permutation Feature Importance
    # ---------------------------------
    pfi = permutation_importance(
        model,
        X_test,
        y_test,
        n_repeats=20,                 # standard compromise accuracy / time
        random_state=rs,
        scoring="roc_auc"             # consistent with ROC analysis
    )

    pfi_list.append(pfi.importances_mean)


# Save all models
joblib.dump(rf_models, "rf_models_TAV.pkl")

rf_models = joblib.load("rf_models_TAV.pkl")

for model in rf_models:

    y_prob = model.predict_proba(X)[:, 1]

    temp_df = pd.DataFrame({
        "SpringID": X.index,   # index used as unique ID
        "RF_prob": y_prob,
        "Impact": y_true
    })

    rf_prob_list.append(temp_df)
    
rf_probs_df = pd.concat(rf_prob_list, ignore_index=True)

# -------------------------------
# Aggregate RF predictions
# -------------------------------
y_pred_rf_full = np.full(len(df), np.nan)
for preds, idx in zip(y_pred_rf_list, test_indices_list):
    y_pred_rf_full[idx] = preds

y_pred_rf_full = y_pred_rf_full.astype(int)

# -------------------------------
# Final statistics (mean ± std)
# -------------------------------
metrics_summary = {
    "Accuracy": (np.mean(acc_list), np.std(acc_list)),
    "Precision": (np.mean(prec_list), np.std(prec_list)),
    "Recall": (np.mean(rec_list), np.std(rec_list)),
    "F1-score": (np.mean(f1_list), np.std(f1_list)),
    "MCC": (np.mean(mcc_list), np.std(mcc_list)),
    "AUC": (np.mean(auc_list), np.std(auc_list)),
}

print("\nPerformance metrics (mean ± std over 100 splits)")
for m, (mean, std) in metrics_summary.items():
    print(f"{m:10s}: {mean:.3f} ± {std:.3f}")

acc_tav, acc_tav_std = metrics_summary["Accuracy"]
prec_tav, prec_tav_std = metrics_summary["Precision"]
rec_tav, rec_tav_std = metrics_summary["Recall"]
f1_tav, f1_tav_std = metrics_summary["F1-score"]
mcc_tav, mcc_tav_std = metrics_summary["MCC"]
auc_tav, auc_tav_std = metrics_summary["AUC"]

# -------------------------------
# DHI predictions
# -------------------------------
y_pred_dhi1 = df["DHI1"].values.astype(int)
y_pred_dhi2 = df["DHI2"].values.astype(int)

# -------------------------------
# Bootstrap comparison
# -------------------------------
def bootstrap_diff(y_true, yA, yB, metric):
    rng = np.random.default_rng(42)
    delta = np.empty(N_BOOTSTRAP)

    for i in range(N_BOOTSTRAP):
        idx = rng.integers(0, len(y_true), len(y_true))
        if metric == f1_score:
            delta[i] = (
                metric(y_true[idx], yA[idx], average="weighted")
                - metric(y_true[idx], yB[idx], average="weighted")
            )
        else:
            delta[i] = metric(y_true[idx], yA[idx]) - metric(y_true[idx], yB[idx])

    return np.mean(delta), *np.percentile(delta, [2.5, 97.5])

# ----------------------------------------
# Mean ROC curve and uncertainty band
# ----------------------------------------
mean_tpr = np.mean(tpr_list, axis=0)
std_tpr = np.std(tpr_list, axis=0)

mean_auc = np.mean(auc_list)
std_auc = np.std(auc_list)

# Ensure ROC ends at (1,1)
mean_tpr[-1] = 1.0

plt.figure(figsize=(7, 6))

# Mean ROC curve
plt.plot(
    mean_fpr,
    mean_tpr,
    color="blue",
    lw=2,
    label=f"RF (AUC = {mean_auc:.2f} ± {std_auc:.2f})"
)

# Error band (±1 standard deviation)
tpr_upper = np.minimum(mean_tpr + std_tpr, 1)
tpr_lower = np.maximum(mean_tpr - std_tpr, 0)
plt.fill_between(mean_fpr, tpr_lower, tpr_upper, color='blue', 
                 alpha=0.2, label='± 1 std. dev.')

# Random classifier reference
plt.plot([0, 1], [0, 1], linestyle="--", lw=1, color="black")

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Mean ROC curve over 100 random splits")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.savefig("ROC_1.png", dpi=600, bbox_inches="tight")
plt.show()

# -----------------------------
# PFI statistics (mean ± std)
# -----------------------------

pfi_array = np.vstack(pfi_list)

pfi_mean = pfi_array.mean(axis=0)
pfi_std = pfi_array.std(axis=0)

pfi_df = pd.DataFrame({
    "Feature": feature_names,
    "PFI_mean": pfi_mean,
    "PFI_std": pfi_std
}).sort_values("PFI_mean", ascending=False)

print("\nTop 10 Permutation Feature Importances")
print(pfi_df.head(10))

# -----------------------------
# PFI plot (Top features)
# -----------------------------
top_n = 15
plt.figure(figsize=(8, 6))

plt.barh(
    pfi_df["Feature"].head(top_n)[::-1],
    pfi_df["PFI_mean"].head(top_n)[::-1],
    xerr=pfi_df["PFI_std"].head(top_n)[::-1],
    alpha=0.7
)

plt.xlabel("Permutation Feature Importance (mean ± std)")
plt.title("Permutation Feature Importance")
plt.grid(axis="x", alpha=0.3)
plt.tight_layout()
plt.savefig("PFI_1.png", dpi=600, bbox_inches="tight")
plt.show()

# -------------------------------
# Aggregate SHAP values
# -------------------------------

shap_values_list = []
all_shap_values = []
 
for model in rf_models:
    explainer = shap.Explainer(model)  # Use an explainer suitable for your model
    shap_values = explainer.shap_values(X)
    shap_values_list.append(shap_values)
    all_shap_values.append(shap_values[1])
 
# Average SHAP values across all models
shap_values_mean = [np.mean(np.array([shap_values[i] for shap_values in shap_values_list]), axis=0)  
                    for i in range(len(shap_values_list[0]))]
mean_shap_values = np.mean(np.abs(np.array(all_shap_values)), axis=0)
 
# Create SHAP plot
fig = plt.figure()
shap.summary_plot(shap_values_mean[1], X, show=False)
fig.savefig("shap_1.png", dpi=600, bbox_inches="tight")
plt.close(fig)
 
# Convert all_shap_values to a numpy array
all_shap_values = np.array(all_shap_values)
 
# Check the shape of SHAP values
print(f"Shape of SHAP values: {all_shap_values.shape}")
 
# Reshape SHAP values matrix to shape (n_predictions * n_models, n_features)
shap_values_reshaped = all_shap_values.reshape(-1, all_shap_values.shape[2])
 
# Compute mean SHAP value for each feature (averaged over all predictions and models)
mean_shap_values = np.mean(np.abs(shap_values_reshaped), axis=0)
 
# Create a table with mean feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Mean SHAP Value': mean_shap_values
}).sort_values(by='Mean SHAP Value', ascending=False)
 
# Display feature importance
print(feature_importance)

# ----------------------------------
# Settings for the boxplot
# ----------------------------------
SPRINGS_PER_FIG = 30
THRESHOLD = 0.5

spring_ids = np.sort(rf_probs_df["SpringID"].unique())
n_figures = int(np.ceil(len(spring_ids) / SPRINGS_PER_FIG))

# ----------------------------------
# Loop over figures
# ----------------------------------
import matplotlib.patches as mpatches

for i in range(n_figures):

    start = i * SPRINGS_PER_FIG
    end = start + SPRINGS_PER_FIG
    spring_subset = spring_ids[start:end]

    df_subset = rf_probs_df[
        rf_probs_df["SpringID"].isin(spring_subset)
    ]

    plt.figure(figsize=(16, 8))

    # Background areas
    plt.axhspan(THRESHOLD, 1.0, color="lightcoral", alpha=0.25)
    plt.axhspan(0.0, THRESHOLD, color="lightblue", alpha=0.25)

    sns.boxplot(
        data=df_subset,
        x="SpringID",
        y="RF_prob",
        hue="Impact",
        palette={1: "red", 0: "blue"},
        linewidth=1,
        showfliers=True
    )

    # Threshold line
    plt.axhline(THRESHOLD, color="k", linestyle="--", linewidth=1)
    plt.ylim(0, 1)
    plt.xlabel("Springs ID")
    plt.ylabel("RF probability")
    plt.title(f"RF Probability Boxplot – Springs {start+1} to {min(end, len(spring_ids))}")

    # Custom legend to match colors
    blue_patch = mpatches.Patch(color='blue', label='Observed no interference')
    red_patch = mpatches.Patch(color='red', label='Observed interference')
    plt.legend(handles=[blue_patch, red_patch], loc="upper right")

    plt.tight_layout()

    plt.savefig(
        f"RF_boxplot_springs_{start+1}_{min(end, len(spring_ids))}.png",
        dpi=600,
        bbox_inches="tight"
    )

    plt.show()
"""
CODE 2
-------------------------------------------------------------------------------
Random Forest vs DHI indices on Sorgenti_GS_nomi dataset.
Includes bootstrap comparison and ROC analysis.
"""

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef,
    roc_curve,
    roc_auc_score
)
from sklearn.inspection import permutation_importance

# -------------------------------
# Settings
# -------------------------------
TEST_SIZE = 0.30
RANDOM_STATES = range(1, 101)
N_BOOTSTRAP = 10_000
ALPHA = 0.05
mean_fpr = np.linspace(0, 1, 100)

# -------------------------------
# Data loading
# -------------------------------
df = pd.read_csv("Sorgenti_GS_nomi.csv", sep=";")

target = "Impatto"
y_true = df[target].values

X = df.drop(columns=[
    target, "Nome", "DHI1", "DHI2",
    "Spring group", "A/P", "Qm (L/s)", "Tip"
])

# -------------------------------
# Containers
# -------------------------------
y_pred_rf_list = []
test_indices_list = []

tpr_list = []
auc_list = []

acc_list = []
prec_list = []
rec_list = []
f1_list = []
mcc_list = []

pfi_list = []              
shap_values_list = []    
models_list = []  

rf_prob_list = []
rf_models = []           

feature_names = X.columns

# -------------------------------
# Random Forest training loop
# -------------------------------
for rs in RANDOM_STATES:

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_true,
        test_size=TEST_SIZE,
        stratify=y_true,
        random_state=rs
    )

    model = RandomForestClassifier(
        random_state=rs,
        max_depth=8,
        n_estimators=490,
        max_features="sqrt",
        min_samples_leaf=1,
        min_samples_split=2
    )

    model.fit(X_train, y_train)
    rf_models.append(model)

    # Predictions
    y_pred = model.predict(X_test)
    y_pred_rf_list.append(y_pred)
    test_indices_list.append(X_test.index)

    y_prob = model.predict_proba(X_test)[:, 1]

    temp_df = pd.DataFrame({
        "SpringID": X_test.index,
        "RF_prob": y_prob,
        "Impact": y_test
    })

    rf_prob_list.append(temp_df)

    # --- Classification metrics ---  
    acc_list.append(accuracy_score(y_test, y_pred))
    prec_list.append(precision_score(y_test, y_pred))
    rec_list.append(recall_score(y_test, y_pred))
    f1_list.append(f1_score(y_test, y_pred))
    mcc_list.append(matthews_corrcoef(y_test, y_pred))

    # --- ROC computation for current split ---  
    y_scores = model.predict_proba(X_test)[:, 1]

    fpr, tpr, _ = roc_curve(y_test, y_scores)
    roc_auc = roc_auc_score(y_test, y_scores)

    # Interpolate TPR over a common FPR grid
    tpr_interp = np.interp(mean_fpr, fpr, tpr)
    tpr_interp[0] = 0.0

    tpr_list.append(tpr_interp)
    auc_list.append(roc_auc)

    # ---------------------------------
    # Permutation Feature Importance
    # ---------------------------------
    pfi = permutation_importance(
        model,
        X_test,
        y_test,
        n_repeats=20,                 # standard compromise accuracy / time
        random_state=rs,
        scoring="roc_auc"             # consistent with ROC analysis
    )

    pfi_list.append(pfi.importances_mean)

# Save all models
joblib.dump(rf_models, "rf_models_GS.pkl")

rf_models = joblib.load("rf_models_GS.pkl")

for model in rf_models:

    y_prob = model.predict_proba(X)[:, 1]

    temp_df = pd.DataFrame({
        "SpringID": X.index,   # index used as unique ID
        "RF_prob": y_prob,
        "Impact": y_true
    })

    rf_prob_list.append(temp_df)
    
rf_probs_df = pd.concat(rf_prob_list, ignore_index=True)

# -------------------------------
# Aggregate RF predictions
# -------------------------------
y_pred_rf_full = np.full(len(df), np.nan)
for preds, idx in zip(y_pred_rf_list, test_indices_list):
    y_pred_rf_full[idx] = preds

y_pred_rf_full = y_pred_rf_full.astype(int)

# -------------------------------
# Final statistics (mean ± std)
# -------------------------------
metrics_summary = {
    "Accuracy": (np.mean(acc_list), np.std(acc_list)),
    "Precision": (np.mean(prec_list), np.std(prec_list)),
    "Recall": (np.mean(rec_list), np.std(rec_list)),
    "F1-score": (np.mean(f1_list), np.std(f1_list)),
    "MCC": (np.mean(mcc_list), np.std(mcc_list)),
    "AUC": (np.mean(auc_list), np.std(auc_list)),
}

print("\nPerformance metrics (mean ± std over 100 splits)")
for m, (mean, std) in metrics_summary.items():
    print(f"{m:10s}: {mean:.3f} ± {std:.3f}")

acc_gs, acc_gs_std = metrics_summary["Accuracy"]
prec_gs, prec_gs_std = metrics_summary["Precision"]
rec_gs, rec_gs_std = metrics_summary["Recall"]
f1_gs, f1_gs_std = metrics_summary["F1-score"]
mcc_gs, mcc_gs_std = metrics_summary["MCC"]
auc_gs, auc_gs_std = metrics_summary["AUC"]

# -------------------------------
# DHI predictions
# -------------------------------
y_pred_dhi1 = df["DHI1"].values.astype(int)
y_pred_dhi2 = df["DHI2"].values.astype(int)

# -------------------------------
# Bootstrap comparison
# -------------------------------
def bootstrap_diff(y_true, yA, yB, metric):
    rng = np.random.default_rng(42)
    delta = np.empty(N_BOOTSTRAP)

    for i in range(N_BOOTSTRAP):
        idx = rng.integers(0, len(y_true), len(y_true))
        if metric == f1_score:
            delta[i] = (
                metric(y_true[idx], yA[idx], average="weighted")
                - metric(y_true[idx], yB[idx], average="weighted")
            )
        else:
            delta[i] = metric(y_true[idx], yA[idx]) - metric(y_true[idx], yB[idx])

    return np.mean(delta), *np.percentile(delta, [2.5, 97.5])

# -------------------------------
# ROC plot
# -------------------------------
mean_tpr = np.mean(tpr_list, axis=0)
mean_auc = np.mean(auc_list)
std_auc = np.std(auc_list)

plt.figure(figsize=(7, 6))
plt.plot(mean_fpr, mean_tpr,
         label=f"RF (AUC = {mean_auc:.2f} ± {std_auc:.2f})",
         linewidth=2)

tpr_upper = np.minimum(mean_tpr + std_tpr, 1)
tpr_lower = np.maximum(mean_tpr - std_tpr, 0)
plt.fill_between(mean_fpr, tpr_lower, tpr_upper, color='blue', 
                 alpha=0.2, label='± 1 std. dev.')

plt.plot([0, 1], [0, 1], "--", lw=1)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC – Sorgenti_GS_nomi dataset")
plt.legend()
plt.grid(alpha=0.3)
plt.savefig("ROC_2.png", dpi=600, bbox_inches="tight")
plt.show()

# -----------------------------
# PFI statistics (mean ± std)
# -----------------------------

pfi_array = np.vstack(pfi_list)

pfi_mean = pfi_array.mean(axis=0)
pfi_std = pfi_array.std(axis=0)

pfi_df = pd.DataFrame({
    "Feature": feature_names,
    "PFI_mean": pfi_mean,
    "PFI_std": pfi_std
}).sort_values("PFI_mean", ascending=False)

print("\nTop 10 Permutation Feature Importances")
print(pfi_df.head(10))

# -----------------------------
# PFI plot (Top features)
# -----------------------------
top_n = 15
plt.figure(figsize=(8, 6))

plt.barh(
    pfi_df["Feature"].head(top_n)[::-1],
    pfi_df["PFI_mean"].head(top_n)[::-1],
    xerr=pfi_df["PFI_std"].head(top_n)[::-1],
    alpha=0.7
)

plt.xlabel("Permutation Feature Importance (mean ± std)")
plt.title("Permutation Feature Importance")
plt.grid(axis="x", alpha=0.3)
plt.tight_layout()
plt.savefig("PFI_2.png", dpi=600, bbox_inches="tight")
plt.show()

# -------------------------------
# Aggregate SHAP values
# -------------------------------

shap_values_list = []
all_shap_values = []
 
for model in rf_models:
    explainer = shap.Explainer(model)  # Use an explainer suitable for your model
    shap_values = explainer.shap_values(X)
    shap_values_list.append(shap_values)
    all_shap_values.append(shap_values[1])
 
# Average SHAP values across all models
shap_values_mean = [np.mean(np.array([shap_values[i] for shap_values in shap_values_list]), axis=0)  
                    for i in range(len(shap_values_list[0]))]
mean_shap_values = np.mean(np.abs(np.array(all_shap_values)), axis=0)
 
# Create SHAP plot
fig = plt.figure()
shap.summary_plot(shap_values_mean[1], X, show=False)
fig.savefig("shap_2.png", dpi=600, bbox_inches="tight")
plt.close(fig)
 
# Convert all_shap_values to a numpy array
all_shap_values = np.array(all_shap_values)
 
# Check the shape of SHAP values
print(f"Shape of SHAP values: {all_shap_values.shape}")
 
# Reshape SHAP values matrix to shape (n_predictions * n_models, n_features)
shap_values_reshaped = all_shap_values.reshape(-1, all_shap_values.shape[2])
 
# Compute mean SHAP value for each feature (averaged over all predictions and models)
mean_shap_values = np.mean(np.abs(shap_values_reshaped), axis=0)
 
# Create a table with mean feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Mean SHAP Value': mean_shap_values
}).sort_values(by='Mean SHAP Value', ascending=False)
 
# Display feature importance
print(feature_importance)

# ----------------------------------
# Settings for the boxplot
# ----------------------------------
SPRINGS_PER_FIG = 30
THRESHOLD = 0.5

spring_ids = np.sort(rf_probs_df["SpringID"].unique())
n_figures = int(np.ceil(len(spring_ids) / SPRINGS_PER_FIG))

# ----------------------------------
# Loop over figures
# ----------------------------------
import matplotlib.patches as mpatches

for i in range(n_figures):

    start = i * SPRINGS_PER_FIG
    end = start + SPRINGS_PER_FIG
    spring_subset = spring_ids[start:end]

    df_subset = rf_probs_df[
        rf_probs_df["SpringID"].isin(spring_subset)
    ]

    plt.figure(figsize=(16, 8))

    # Background areas
    plt.axhspan(THRESHOLD, 1.0, color="lightcoral", alpha=0.25)
    plt.axhspan(0.0, THRESHOLD, color="lightblue", alpha=0.25)

    sns.boxplot(
        data=df_subset,
        x="SpringID",
        y="RF_prob",
        hue="Impact",
        palette={1: "red", 0: "blue"},
        linewidth=1,
        showfliers=True
    )

    # Threshold line
    plt.axhline(THRESHOLD, color="k", linestyle="--", linewidth=1)
    plt.ylim(0, 1)
    plt.xlabel("Springs ID")
    plt.ylabel("RF probability")
    plt.title(f"RF Probability Boxplot – Springs {start+1} to {min(end, len(spring_ids))}")

    # Custom legend to match colors
    blue_patch = mpatches.Patch(color='blue', label='Observed no interference')
    red_patch = mpatches.Patch(color='red', label='Observed interference')
    plt.legend(handles=[blue_patch, red_patch], loc="upper right")

    plt.tight_layout()

    plt.savefig(
        f"GS_boxplot_springs_{start+1}_{min(end, len(spring_ids))}.png",
        dpi=600,
        bbox_inches="tight"
    )

    plt.show()
"""
CODE 3
-------------------------------------------------------------------------------
Random Forest vs DHI indices on the combined dataset.
Includes bootstrap comparison and ROC analysis.
"""
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef,
    roc_curve,
    roc_auc_score
)
from sklearn.inspection import permutation_importance

# -------------------------------
# Settings
# -------------------------------
TEST_SIZE = 0.30
RANDOM_STATES = range(1, 101)
N_BOOTSTRAP = 10_000
ALPHA = 0.05
mean_fpr = np.linspace(0, 1, 100)

# -------------------------------
# Data loading and merging
# -------------------------------
df1 = pd.read_csv("TAV_Machine.csv", sep=";")
df1 = df1.drop(df1.index[160:167]).reset_index(drop=True)

df2 = pd.read_csv("Sorgenti_GS_nomi.csv", sep=";").drop(columns=["Nome"])

df = pd.concat([df1, df2], ignore_index=True)

target = "Impatto"
y_true = df[target].values

X = df.drop(columns=[
    target, "Tipo", "Spring group", "A/P", "Qm (L/s)",
    "DHI1", "DHI2",
    "Fault distance (m)", "Fault distance 2 (m)",
    "Numero", "Tip"
])
# -------------------------------
# Containers
# -------------------------------
y_pred_rf_list = []
test_indices_list = []

tpr_list = []
auc_list = []

acc_list = []
prec_list = []
rec_list = []
f1_list = []
mcc_list = []

pfi_list = []              
shap_values_list = []    
models_list = []  

rf_prob_list = []
rf_models = []           

feature_names = X.columns

# -------------------------------
# Random Forest training loop
# -------------------------------
for rs in RANDOM_STATES:

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_true,
        test_size=TEST_SIZE,
        stratify=y_true,
        random_state=rs
    )

    model = RandomForestClassifier(
        random_state=rs,
        max_depth=8,
        n_estimators=490,
        max_features="sqrt",
        min_samples_leaf=1,
        min_samples_split=2
    )

    model.fit(X_train, y_train)
    rf_models.append(model)

    # Predictions
    y_pred = model.predict(X_test)
    y_pred_rf_list.append(y_pred)
    test_indices_list.append(X_test.index)

    y_prob = model.predict_proba(X_test)[:, 1]

    temp_df = pd.DataFrame({
        "SpringID": X_test.index,
        "RF_prob": y_prob,
        "Impact": y_test
    })

    rf_prob_list.append(temp_df)

    # --- Classification metrics ---  
    acc_list.append(accuracy_score(y_test, y_pred))
    prec_list.append(precision_score(y_test, y_pred))
    rec_list.append(recall_score(y_test, y_pred))
    f1_list.append(f1_score(y_test, y_pred))
    mcc_list.append(matthews_corrcoef(y_test, y_pred))

    # --- ROC computation for current split ---  
    y_scores = model.predict_proba(X_test)[:, 1]

    fpr, tpr, _ = roc_curve(y_test, y_scores)
    roc_auc = roc_auc_score(y_test, y_scores)

    # Interpolate TPR over a common FPR grid
    tpr_interp = np.interp(mean_fpr, fpr, tpr)
    tpr_interp[0] = 0.0

    tpr_list.append(tpr_interp)
    auc_list.append(roc_auc)

    # ---------------------------------
    # Permutation Feature Importance
    # ---------------------------------
    pfi = permutation_importance(
        model,
        X_test,
        y_test,
        n_repeats=20,                 # standard compromise accuracy / time
        random_state=rs,
        scoring="roc_auc"             # consistent with ROC analysis
    )

    pfi_list.append(pfi.importances_mean)

# Save all models
joblib.dump(rf_models, "rf_models_Total.pkl")

rf_models = joblib.load("rf_models_Total.pkl")

for model in rf_models:

    y_prob = model.predict_proba(X)[:, 1]

    temp_df = pd.DataFrame({
        "SpringID": X.index,   # index used as unique ID
        "RF_prob": y_prob,
        "Impact": y_true
    })

    rf_prob_list.append(temp_df)
    
rf_probs_df = pd.concat(rf_prob_list, ignore_index=True)
# -------------------------------
# Aggregate RF predictions
# -------------------------------
y_pred_rf_full = np.full(len(df), np.nan)
for preds, idx in zip(y_pred_rf_list, test_indices_list):
    y_pred_rf_full[idx] = preds

y_pred_rf_full = y_pred_rf_full.astype(int)

# -------------------------------
# Final statistics (mean ± std)
# -------------------------------
metrics_summary = {
    "Accuracy": (np.mean(acc_list), np.std(acc_list)),
    "Precision": (np.mean(prec_list), np.std(prec_list)),
    "Recall": (np.mean(rec_list), np.std(rec_list)),
    "F1-score": (np.mean(f1_list), np.std(f1_list)),
    "MCC": (np.mean(mcc_list), np.std(mcc_list)),
    "AUC": (np.mean(auc_list), np.std(auc_list)),
}

print("\nPerformance metrics (mean ± std over 100 splits)")
for m, (mean, std) in metrics_summary.items():
    print(f"{m:10s}: {mean:.3f} ± {std:.3f}")

acc_comb, acc_comb_std = metrics_summary["Accuracy"]
prec_comb, prec_comb_std = metrics_summary["Precision"]
rec_comb, rec_comb_std = metrics_summary["Recall"]
f1_comb, f1_comb_std = metrics_summary["F1-score"]
mcc_comb, mcc_comb_std = metrics_summary["MCC"]
auc_comb, auc_comb_std = metrics_summary["AUC"]
# -------------------------------
# DHI predictions
# -------------------------------
y_pred_dhi1 = df["DHI1"].values.astype(int)
y_pred_dhi2 = df["DHI2"].values.astype(int)

# -------------------------------
# Bootstrap comparison
# -------------------------------
def bootstrap_diff(y_true, yA, yB, metric):
    rng = np.random.default_rng(42)
    delta = np.empty(N_BOOTSTRAP)

    for i in range(N_BOOTSTRAP):
        idx = rng.integers(0, len(y_true), len(y_true))
        if metric == f1_score:
            delta[i] = (
                metric(y_true[idx], yA[idx], average="weighted")
                - metric(y_true[idx], yB[idx], average="weighted")
            )
        else:
            delta[i] = metric(y_true[idx], yA[idx]) - metric(y_true[idx], yB[idx])

    return np.mean(delta), *np.percentile(delta, [2.5, 97.5])

# -------------------------------
# ROC plot
# -------------------------------
mean_tpr = np.mean(tpr_list, axis=0)
mean_auc = np.mean(auc_list)
std_auc = np.std(auc_list)

plt.figure(figsize=(7, 6))
plt.plot(mean_fpr, mean_tpr,
         label=f"RF (AUC = {mean_auc:.2f} ± {std_auc:.2f})",
         linewidth=2)
tpr_upper = np.minimum(mean_tpr + std_tpr, 1)
tpr_lower = np.maximum(mean_tpr - std_tpr, 0)
plt.fill_between(mean_fpr, tpr_lower, tpr_upper, color='blue', 
                 alpha=0.2, label='± 1 std. dev.')
plt.plot([0, 1], [0, 1], "--", lw=1)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC – Combined dataset")
plt.legend()
plt.grid(alpha=0.3)
plt.savefig("ROC_3.png", dpi=600, bbox_inches="tight")
plt.show()

# -----------------------------
# PFI statistics (mean ± std)
# -----------------------------

pfi_array = np.vstack(pfi_list)

pfi_mean = pfi_array.mean(axis=0)
pfi_std = pfi_array.std(axis=0)

pfi_df = pd.DataFrame({
    "Feature": feature_names,
    "PFI_mean": pfi_mean,
    "PFI_std": pfi_std
}).sort_values("PFI_mean", ascending=False)

print("\nTop 10 Permutation Feature Importances")
print(pfi_df.head(10))

# -----------------------------
# PFI plot (Top features)
# -----------------------------
top_n = 15
plt.figure(figsize=(8, 6))

plt.barh(
    pfi_df["Feature"].head(top_n)[::-1],
    pfi_df["PFI_mean"].head(top_n)[::-1],
    xerr=pfi_df["PFI_std"].head(top_n)[::-1],
    alpha=0.7
)

plt.xlabel("Permutation Feature Importance (mean ± std)")
plt.title("Permutation Feature Importance")
plt.grid(axis="x", alpha=0.3)
plt.tight_layout()
plt.savefig("PFI_3.png", dpi=600, bbox_inches="tight")
plt.show()

# -------------------------------
# Aggregate SHAP values
# -------------------------------

shap_values_list = []
all_shap_values = []
 
for model in rf_models:
    explainer = shap.Explainer(model)  # Use an explainer suitable for your model
    shap_values = explainer.shap_values(X)
    shap_values_list.append(shap_values)
    all_shap_values.append(shap_values[1])
 
# Average SHAP values across all models
shap_values_mean = [np.mean(np.array([shap_values[i] for shap_values in shap_values_list]), axis=0)  
                    for i in range(len(shap_values_list[0]))]
mean_shap_values = np.mean(np.abs(np.array(all_shap_values)), axis=0)
 
# Create SHAP plot
fig = plt.figure()
shap.summary_plot(shap_values_mean[1], X, show=False)
fig.savefig("shap_3.png", dpi=600, bbox_inches="tight")
plt.close(fig)
 
# Convert all_shap_values to a numpy array
all_shap_values = np.array(all_shap_values)
 
# Check the shape of SHAP values
print(f"Shape of SHAP values: {all_shap_values.shape}")
 
# Reshape SHAP values matrix to shape (n_predictions * n_models, n_features)
shap_values_reshaped = all_shap_values.reshape(-1, all_shap_values.shape[2])
 
# Compute mean SHAP value for each feature (averaged over all predictions and models)
mean_shap_values = np.mean(np.abs(shap_values_reshaped), axis=0)
 
# Create a table with mean feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Mean SHAP Value': mean_shap_values
}).sort_values(by='Mean SHAP Value', ascending=False)
 
# Display feature importance
print(feature_importance)

# ----------------------------------
# Settings for the boxplot
# ----------------------------------
SPRINGS_PER_FIG = 30
THRESHOLD = 0.5

spring_ids = np.sort(rf_probs_df["SpringID"].unique())
n_figures = int(np.ceil(len(spring_ids) / SPRINGS_PER_FIG))

# ----------------------------------
# Loop over figures
# ----------------------------------
import matplotlib.patches as mpatches

for i in range(n_figures):

    start = i * SPRINGS_PER_FIG
    end = start + SPRINGS_PER_FIG
    spring_subset = spring_ids[start:end]

    df_subset = rf_probs_df[
        rf_probs_df["SpringID"].isin(spring_subset)
    ]

    plt.figure(figsize=(16, 8))

    # Background areas
    plt.axhspan(THRESHOLD, 1.0, color="lightcoral", alpha=0.25)
    plt.axhspan(0.0, THRESHOLD, color="lightblue", alpha=0.25)

    sns.boxplot(
        data=df_subset,
        x="SpringID",
        y="RF_prob",
        hue="Impact",
        palette={1: "red", 0: "blue"},
        linewidth=1,
        showfliers=True
    )

    # Threshold line
    plt.axhline(THRESHOLD, color="k", linestyle="--", linewidth=1)
    plt.ylim(0, 1)
    plt.xlabel("Springs ID")
    plt.ylabel("RF probability")
    plt.title(f"RF Probability Boxplot – Springs {start+1} to {min(end, len(spring_ids))}")

    # Custom legend to match colors
    blue_patch = mpatches.Patch(color='blue', label='Observed no interference')
    red_patch = mpatches.Patch(color='red', label='Observed interference')
    plt.legend(handles=[blue_patch, red_patch], loc="upper right")

    plt.tight_layout()

    plt.savefig(
        f"Total_boxplot_springs_{start+1}_{min(end, len(spring_ids))}.png",
        dpi=600,
        bbox_inches="tight"
    )

    plt.show()
# -------------------------------
# Summary metrics
# -------------------------------
summary = pd.DataFrame({
    "Dataset": ["TAV", "Sorgenti_GS", "Combined"],
    "Accuracy": [acc_tav, acc_gs, acc_comb],
    "Precision": [prec_tav, prec_gs, prec_comb],
    "Recall": [rec_tav, rec_gs, rec_comb],
    "F1": [f1_tav, f1_gs, f1_comb],
    "MCC": [mcc_tav, mcc_gs, mcc_comb],
    "AUC": [auc_tav, auc_gs, auc_comb],
})

print(summary)
